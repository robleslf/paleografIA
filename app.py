import streamlit as st
from src.query_engine import QueryEngine # Importamos nuestro motor

# --- Configuraci칩n de la P치gina de Streamlit ---
st.set_page_config(
    page_title="paleografIA",
    page_icon="游닆",
    layout="wide", # Usamos un layout ancho para m치s espacio
    initial_sidebar_state="expanded"
)

# --- Estado de la Sesi칩n ---
# Streamlit vuelve a ejecutar el script con cada interacci칩n.
# Usamos st.session_state para guardar objetos y que no se recarguen constantemente.
if 'engine' not in st.session_state:
    # Inicializamos el motor de consulta SOLO la primera vez que se carga la app
    st.session_state.engine = QueryEngine()

if 'messages' not in st.session_state:
    # Inicializamos el historial de la conversaci칩n
    st.session_state.messages = []

# --- Dise침o de la Interfaz ---

# 1. Barra Lateral (Sidebar)
with st.sidebar:
    st.title("游닆 paleografIA")
    st.markdown("""
    **Asistente de Investigaci칩n Inteligente para las Actas Hist칩ricas de la Junta General del Principado de Asturias (1594-1718).**
    
    Realice consultas en lenguaje natural sobre el corpus documental. El sistema decidir치 si usar el grafo de conocimiento para preguntas espec칤ficas o la b칰squeda de texto para preguntas abiertas.
    """)
    st.markdown("---")
    
    # Mostramos las estad칤sticas del grafo
    st.subheader("Estad칤sticas del Corpus (Muestra)")
    if st.session_state.engine.graph:
        num_nodos = st.session_state.engine.graph.number_of_nodes()
        num_relaciones = st.session_state.engine.graph.number_of_edges()
        st.metric(label="Entidades Reconocidas (Nodos)", value=num_nodos)
        st.metric(label="Relaciones Detectadas (Aristas)", value=num_relaciones)
    else:
        st.error("No se pudo cargar el grafo de conocimiento.")

    st.markdown("---")
    st.info("Proyecto en fase de prototipo. Desarrollado por [Tu Nombre/Proyecto].")

# 2. 츼rea Principal (Chat)
st.header("Sistema de Consulta Interactivo")

# Mostrar mensajes del historial
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Capturar la nueva pregunta del usuario
if prompt := st.chat_input("Realice su consulta aqu칤..."):
    # A침adir la pregunta del usuario al historial y mostrarla
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generar y mostrar la respuesta de la IA
    with st.chat_message("assistant"):
        with st.spinner("Procesando consulta..."):
            # 춰Aqu칤 ocurre la magia! Llamamos a nuestro motor.
            response = st.session_state.engine.query(prompt)
            st.markdown(response)
    
    # A침adir la respuesta de la IA al historial
    st.session_state.messages.append({"role": "assistant", "content": response})
